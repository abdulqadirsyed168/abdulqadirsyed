Authentication Method
1.System Assigned Managed Identity(SAMI): It's default created and attaching to ADB to ADF, No scope of delete in future. owent get deleted it does not have expire date.
2.User Assigned Managed Identity(UAMI): There can be scope to be delete by some one in future. all pipelines will stop our linked services will fail.
3.Access token : But it has expire

ADF to ADLS Gen 2 = System Assigned Managed Identity(SAMI)
ADF to ADB = System Assigned Managed Identity(SAMI)
ADF to Azure Databricks Delta lake = System Assigned Managed Identity(SAMI)
ADF to Azure key vault = System Assigned Managed Identity(SAMI)
ADB to ADLS Gen 2 = SPN(Service principal)

System Assigned Managed Identity(SAMI) has to give access from Access control(IAM) afor every Dashboard like ADB,ADLS,KEY VAULT

Linked service ADF to Azure Databricks Delta lake
Go to Databricks workspace
click access control(IAM)
click add = Add role assignment
goto priviledged administrator roles = select contributer
click next
click managed identity
select members
select your ADF
create

Linked service ADF to ADB
Go to Databricks workspace
click access control(IAM)
click add = Add role assignment
goto priviledged administrator roles = select contributer
click next
click managed identity
select members
select your ADF
create

Linked service ADF to ADLS
Go to ADLS workspace
click access control(IAM)
click add = Add role assignment
goto job function roles = select storage blob data contributor
click next
click managed identity
select members
select your ADF
create

Linked service Azure key vault to ADF,ADB,ADLS
Go to keyvaults dashboard
click your keyvault
click on access policies
in permission
goto secret permission
select get,list,set
next
goto principal
search for SAMI
create
Now create secrets for mysql,SFTP 
Go to Azure key vault workspace
click access control(IAM)
click add = Add role assignment
goto job function roles = select storage blob data contributor
click next
click managed identity
select members
select your ADF
create

Linked service ADLS to ADB(SPN)
Go to Azure active directory(Entra ID)
click on app registrations
click on New registration
click register
copy application id,tenant id
click certificates and secrets
create it
copy the value
Go to ADLS workspace
click access control(IAM)
click add = Add role assignment
goto job function roles = select storage blob data contributor
click next
click SERVICE PRINCIPAL
select members
select your SPN
create
Go to databricks
copy paste storagename,application id,tenant id
and copy paste it to cluster configuration in spark config

1.Authentication method = System Assigned Managed Identity(SAMI)
2.choose your databricks workspace
3.CLUSTER ID = Launch Databricks workspace,go to compute,click cluster, goto JDBC/ODBC, goto HTTPS path copy end part

AZURE KEY VAULTS
Databricks wrokspace_id/createScope
It will ask for scope name 
DNS name
ResourceID THIS details can be retrive from azure key vaults
navigate to azure key valuts go to properties and Vault URI is nothing but a DNS name and resource_id
display(dbutils.secrets,listScopes())
dbutils.secrets.get(scope = "", key = "")


