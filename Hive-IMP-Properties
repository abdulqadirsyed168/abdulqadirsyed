# For data load from local
load data local inpath 'file:///tmp/hive_class/depart_data.csv' into table department_data; 

# Load data from hdfs location
load data inpath '/tmp/hive_data_class_2/' into table department_data_from_hdfs;

# Display column name
set hive.cli.print.header = true;

# command to create identical table
create table sales_data_v2_bkup as select * from sales_data_v2;

# To skip first row header in csv
tblproperties ("skip.header.line.count" = "1");

# load data in parquet file for serlizied and deserlizied for serde
from sales_data_v2 insert overwrite table sales_data_pq_final select *;

# TO CREATE STATIC_PARTITION OR DYNAMIC_PARTITION TABLE EXAMPLE 
create table customer_static_part
(
customer_id INT,
customer_name VARCHAR(50),
contact_name VARCHAR(50),
address VARCHAR(50),
city VARCHAR(50),
postal_code VARCHAR(50)
)
partitioned by (country VARCHAR(50));

# TO CREATE TABLE FOR BUCKETING:- TABLE EXAMPLE
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set hive.enforce.bucketing = true;

 create table customer_bucketing
    > (
    > customer_id INT,
    > customer_name VARCHAR(50),
    > contact_name VARCHAR(50),
    > address VARCHAR(50),
    > city VARCHAR(50),
    > postal_code VARCHAR(50),
    > country VARCHAR(50))
    > clustered by (customer_name) into 4 buckets; #NEED TO MENTIONS ALL COLUMNS WHILE CREATING TABLE 


# set this property if doing static partition
NO NEED TO SET THIS PROPERTIE HIVE BYDEFAULT RUNS ON STATIC PART set hive.mapred.mode=strict;

# set this property for dynamic partioning
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set hive.exec.enforce.bucketing = true;


# load data in static partition table
insert overwrite table sales_data_static_part partition(country = 'USA') select ordernumber,quantityordered,sales,year_id from sales_ord
er_data_orc where country = 'USA';

# load data in dynamic partition table
insert overwrite table sales_data_dynamic_part partition(country) select ordernumber,quantityordered,sales,year_id,country from sales_or
der_data_orc;

# load data in bucketing table
insert overwrite table customers_bucketing select * from customers;



# set this property for bucketing
set hive.enforce.bucketing=true;

# Reduce-Side Join
SET hive.auto.convert.join=false;

# Map-Side join
SET hive.auto.convert.join=true;

# Bucket map join
SET hive.auto.convert.join=true;
SET hive.optimize.bucketmapjoin=true;

# Sort-Merge Bucket map join
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join=true;
set hive.enforce.sortmergebucketmapjoin=false;



row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' #TO INSERT CSV DATA FILEFORMAT                                                                          
    > with serdeproperties (                                                                                                                  
    >  "separatorChar" = ",",                                                                                                                 
    >  "quoteChar" = "\"",                                                                                                                    
    >  "escapeChar" = "\\"                                                                                                                    
    > )
    
    
In order to set a constant number of reducers:                                                                                                
  set mapreduce.job.reduces=<number> 
  
 # change number of reducers to 3
 
 set mapreduce.job.reduces=3;


create table department_data #INTERNAL TABLES                                                                                                           
    > (                                                                                                                                       
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int)                                                                                                                             
    > row format delimited                                                                                                                    
    > fields terminated by ','
    
# Create external table 
create external table department_data_external                                                                                          
    > (                                                                                                                                       
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int                                                                                                                              
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > location '/tmp/hive_data_class_2/'; 
    

# create a table which will store data in parquet

create table sales_data_pq_final                                                                                                        
    > (                                                                                                                                       
    > product_type string,                                                                                                                    
    > total_sales int                                                                                                                         
    > )                                                                                                                                       
    > stored as parquet;  
    
# load data in parquet file
from sales_data_v2 insert overwrite table sales_data_pq_final select *;


# PARTITION WITH BUCKETING:-
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set hive.enforce.bucketing = true;

 create table customer_partition_bucketing
    > (
    > customer_id INT,
    > customer_name VARCHAR(50),
    > contact_name VARCHAR(50),
    > address VARCHAR(50),
    > city VARCHAR(50),
    > postal_code VARCHAR(50))
    > partitioned by (country VARCHAR(50))
    > clustered by (customer_name) into 4 buckets;
  
  # load data
 insert overwrite table customer_partition_bucketing partition(country) select * from customers;








