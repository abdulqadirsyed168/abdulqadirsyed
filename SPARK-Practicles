COMMAND TO CHECK SPARK WEB URL
sc.uiWebUrl

SPARK SUBMIT COMMAND
./bin/spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  [application-arguments]
  
SPARK SUMIT COMMAND YARN
./bin/spark2-submit \
   --master yarn \
   --deploy-mode cluster \
   --driver-memory 8g \
   --executor-memory 16g \
   --executor-cores 2  \
   --class org.apache.spark.examples.SparkPi \
   /spark-home/examples/jars/spark-examples_versionxx.jar 80
   

# RUNNING SPARK APPLICATION ON STANDALONE CLUSTER
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://192.168.231.132:7077 \
  --deploy-mode cluster \
  --executor-memory 5G \
  --executor-cores 8 \
  /spark-home/examples/jars/spark-examples_versionxx.jar 80


CREATE SPARK CONTEXT
conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf)

CREATE SPARK SESSION
from pyspark.sql import SparkSession
spark = SparkSession.builder.master(local[*]).appName().config().getOrCreate()

ENABLE HIVE IN SPARK
spark = SparkSession.builder()
      .master("local[1]")
      .appName("SparkByExamples.com")
      .config("spark.sql.warehouse.dir", "<path>/spark-warehouse")
      .enableHiveSupport()
      .getOrCreate();
  
 SPARK SUBMIT AFTER CREATING ALL CODE IN .py file and we need to run in terminal CMD promt spark-submit
 spark-submit script.py &> results.txt

HOW MANY WAYS CAN WE CREATE RDD

rdd = sc.parallelize(range(10))
rrd1 = spark.sparkContext.parallelize(range(10))

HOW MANY WAYS CAN WE ABLE TO CONVERT DF TO RDD
r1 = df.rdd

HOW MANY WAYS CAN WE ABLE TO CONVERT RDD TO DF

df = rdd1.toDF(schema = [])
df = spark.createDataFrame(rdd1, schema = [])

TWO WAYS WE CAN ABLE TO CREATE DATAFRAME
df = spark.createDataFrame(var1, schema = [])
df = sqlContext.createDataFrame

TO CONVERT DATAFRAME TO SQL
df.createOrReplaceTempView('table_name')

Cache & Persistence
import org.apache.spark.storage.
df.persis(StorageLevel.MEMORY_ONLY)
df.unpersist()

READ DIFFERENT FILE FORMAT IN PYSPARK

Books_df = spark.read.format("xml").option("rootTag", "books").option("rowTag", "book").load("dbfs:/FileStore/Tables/XmlFolder/Books.xml")-----XML FORMAT
Filename_df = spark.read.json("/tmp/Filename.type")-------JSON SINGLE LINE FORMAT
Filename_df = spark.read.option("multiline", "true").json("/tmp/Filename.type")---------------JSON MULTI LINE FORMAT
userdata2_df = spark.read.parquet("dbfs:/FileStore/Tables/ParquetFolder/userdata2.parquet")






dbutils.fs.mkdirs("/FileStore/Tables/TextFolder") #MAKE Directory

# This two ways we can able to read a file 
Emp_df = spark.read.csv("dbfs:/FileStore/Tables/TextFolder/emp.txt", header=True, inferSchema=True)
Dep_df = spark.read.option("header",True).option("inferSchema",True).csv("dbfs:/FileStore/Tables/TextFolder/dep.txt")


from pyspark.sql.functions import col
from pyspark.sql.functions import *
from pyspark.sql.types import StructType,StructField,StructType,IntegerType,StringType
from pyspark.sql.window import Window

Performance Optimization
Repartion
Coalesce
Partion
Bucketing

sc.defaultParallelism to check partitons
rdd.getNumPartitions()
.glom() To check partition wise data df.glom().collect()

Repartion
It involves shuffeling
df1 = df.repartition(20)
df1.rdd.glom().collect()
df2 = df.repartition(2)
df2.rdd.glom().collect()

Coalesce
It does not involve shuffeling 
df2 = df.coalesce(2)
df2.rdd.glom().collect()


PARTITION
Partion will be perfom on low cordinality means less unique values for a column
df.write.partitionBy("column_name").csv(Path)
df.write.option("header", True).partitionBy("year").mode("overwrite").csv("path")
df.write.option("header", True).partitionBy("year","sex").mode("overwrite").csv("path")
df.write.option("header", True).option("maxRecordsPerFile", 4200).partitionBy("year").mode("overwrite").csv("path")

BUCKETING
Bucketing will be perfom on high cordinality means all unique values for a column, based on hash value record will placed into bucketing
df.write.bucketBy(10,"column_name").csv(Path)

PIVOT
pivot_df = df.groupby("col_name").pivot("col_name").sum("col_name") - This command will convert rows to columns



Dep_df.select("*").show()
Emp_df.select("EMPLOYEE_ID","FIRST_NAME").show()
Emp_df.select(Emp_df.EMPLOYEE_ID,Emp_df.FIRST_NAME).show()
Emp_df.select(Emp_df['EMPLOYEE_ID'],Emp_df['FIRST_NAME']).show()
Emp_df.select(col("employee_id"),col("first_name")).show()
Emp_df.select(col("employee_id").alias("Emp_id"),col("first_name").alias("F_name")).show()

Emp_df.select("employee_id","first_name","salary").withColumn("New_salary",col("SALARY")+1000).show()
Emp_df.withColumn("New_salary",col("salary")+1000).select("employee_id","first_name","new_salary").show()
Emp_df.withColumn("salary",col("salary") - 1000).select("employee_id","first_name","salary").show()
Emp_df.withColumnRenamed("salary","emp_salary").show()
from pyspark.sql.functions import spark_partition_id
df.withColumn("partitionId", spark_partition_id())
df.repartition(20).withColumn("partitionId", spark_partition_id())

Emp_df.drop("commission_pct").show()
Emp_df.filter(col("salary") < 5000).show()
Emp_df.filter(col("salary") > 5000).show(100)
Emp_df.filter(col("salary") < 5000).select("employee_id","first_name","last_name","salary").show(100)
Emp_df.filter(col("department_id") == 50).select("employee_id","first_name","last_name","salary").show(100)
Emp_df.filter("department_id <> 50").select("employee_id","first_name","last_name","salary","department_id").show(100)
Emp_df.filter("department_id != 50").select("employee_id","first_name","last_name","salary","department_id").show(100)
Emp_df.filter("department_id != 50").filter(col("salary") > 5000).select("employee_id","first_name","last_name","salary","department_id").show(100)
Emp_df.filter("department_id == 50 and salary > 5000").select("employee_id","first_name","last_name","salary","department_id").show(100)

Emp_df.distinct().show(100)
Emp_df.dropDuplicates().show(100)
Emp_df.dropDuplicates(["department_id", "hire_date"]).show(100)
Emp_df.dropDuplicates(["department_id", "hire_date"]).select("employee_id","department_id","hire_date").show(100)


Emp_df.count()
Emp_df.select(count("salary")).show()
Emp_df.select(count("salary").alias("Total_count")).show()
Emp_df.select(max("salary").alias("Max_salary")).show()
Emp_df.select(min("salary").alias("Min_salary")).show()
Emp_df.select(avg("salary").alias("Avg_salary")).show()
Emp_df.select(sum("salary").alias("Sum_salary")).show()

Emp_df.select("*").orderBy("salary").show()
Emp_df.select("*").orderBy(col("salary").desc()).show()
Emp_df.select("*").orderBy(col("salary").desc(),col("department_id").asc()).show()
Emp_df.select("EMPLOYEE_ID","FIRST_NAME","DEPARTMENT_ID","SALARY").orderBy(col("salary").desc(),col("department_id").asc()).show()

Emp_df.groupBy("department_id").sum("salary").show(100)
Emp_df.groupBy("department_id").max("salary").show(100)
Emp_df.groupBy("department_id").min("salary").show(100)
Emp_df.groupBy("department_id","job_id").sum("salary").show(100)
Emp_df.groupBy("department_id","job_id").sum("salary","employee_id").show(100)

Emp_df.groupBy("department_id").agg(sum("salary").alias("Sum_salary"),max("salary").alias("Max_salary"),min("salary").alias("Min_salary"),
avg("salary").alias("Avg_salary")).show(100)
Emp_df.groupBy("department_id").agg(sum("salary").alias("Sum_salary"),max("salary").alias("Max_salary"),min("salary").alias("Min_salary"),
avg("salary").alias("Avg_salary")).where(col("Max_salary") >= 10000).show() 

Emp_df.withColumn("Emp_Grade", when( col("salary") > 15000, "A").when( ( col("salary") >= 10000) & ( col("salary") < 15000), "B").otherwise("C")).show(100)


Emp_df.createOrReplaceTempView("Employee")
spark.sql("select * from Employee limit 5").show()
spark.sql("select department_id,sum(salary) as total_salary from Employee group by department_id").show()
spark.sql("select employee_id, department_id, rank() over(partition by department_id order by salary desc) as rank_salary from employee").show()

Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "inner").show()
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "inner").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "left").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "right").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "fullouter").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
from pyspark.sql.functions import broadcast
Emp_df.join(broadcast((Dep_df), Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID).show()


windowSpec = Window.partitionBy("DEPARTMENT_ID").orderBy("SALARY")
Emp_df.withColumn("salary_rank", rank().over(windowSpec)).select("DEPARTMENT_ID","SALARY","salary_rank").show(100)

windowSpec = Window.partitionBy("DEPARTMENT_ID").orderBy("SALARY")
Emp_df.withColumn("sum", sum("SALARY").over(windowSpec)).select("DEPARTMENT_ID","SALARY","sum").show(100)

windowSpec = Window.partitionBy("DEPARTMENT_ID")
Emp_df.withColumn("sum", sum("SALARY").over(windowSpec)).select("DEPARTMENT_ID","SALARY","sum").show(100)

WRITE modes in spark
1.Overwrite - First it will delete the existing directory and then it will create a new directory
2.Append - Appends the output files to the directory if it already exists
3.Ignore - If output directory exists, it won't do anything
4.Error - Throws an error if the directory already exists

READ modes in spark
1.Permissive - It sets all fields to Null, When it encounters a corrupted record, it's the default read mode and places the corrupted record in a string column called '_corrupt_record'
2.dropMalformed - It will simply ignore the corrupted record
3.failFast - Whenever the corrupted record is found, it will raise an exception and it won't display any output
df = spark.read.option("header",True).option("mode","permissive").load('path')


FILTER CONDITIONS

display(name_df.filter(name_df.columnname ==, >, <, >=, <=, != ))

display(name_df.filter((name_df.columnname =="string value") & (name_df.columnname == "string value")) ) #AND
display(name_df.filter((name_df.columnname =="string value") | (name_df.columnname == "string value")) ) #OR

display(name_df.filter((name_df.columnname.startswith("value"))) ) 
display(name_df.filter((name_df.columnname.endswith("value"))) )
display(name_df.filter((name_df.columnname.contains("value"))) ) 

display(name_df.filter((name_df.columnname.isNull())) )
display(name_df.filter((name_df.columnname.isNotNull())) )

display(name_df.filter((name_df.columnname.isin(1,2,3))) )
display(name_df.filter((~name_df.columnname.isin())) )

display(name_df.filter((name_df.columnname.like("%stringname%"))) )
display(name_df.filter((name_df.columnname.like("%stringname"))) )
display(name_df.filter((name_df.columnname.like("stringname%"))) )

Multiple ways of creating DELTA TABLES by SPARK,SQL,DATAFRAME styles

SPARK
from delta.tables import *
DeltaTable.create(spark).tableName("database.table_name").addColumn("").property("").location("").execute()
DeltaTable.createIfNotExists(spark).tableName("database.table_name").addColumn("").property("").location("").execute()
DeltaTable.createOrReplace(spark).tableName("database.table_name").addColumn("").property("").location("").execute() - It will delete old one and create new one

SQL
CREATE TABLE Table_name(col1 datatype,col2 datatype) USING DELTA
CREATE TABLE IF NOT EXISTS Table_name(col1 datatype,col2 datatype) USING DELTA
CREATE OR REPLACE TABLE Table_name(col1 datatype,col2 datatype) USING DELTA LOCATION "path"

Dataframe
df.write.format("delta").saveAsTable("database_name.table_name")

Delta Instance
deltaInstance1 = DeltaTable.forPath(spark, "path")
display(deltaInstance1.toDF())
deltaInstance1.delete("emp_id = 200")
display(deltaInstance1.history())

deltaInstance2 = DeltaTable.forName(spark, "table_name")
display(deltaInstance2.toDF())
deltaInstance2.delete("emp_id = 200")
display(deltaInstance2.history())

Delta Lake: Restore Command
table_name.restoreToVersion(version_num)
table_name.restoreToTimestamp(timestamp)

Delta Lake: Optimize Command - File Compaction
OPTIMIZE table_name

Delta Lake: Vacuum Command
VACCUM table_name DRY RUN
VACCUM table_name RETAIN HOURS DRY RUN

Delta: Z-Order Command
OPTIMIZE table_name
ZORDER col_name

Delta: Schema Evolution - MergeSchema
df.write.option("mergeSchema", "true").format("delta").mode("append").saveAsTable("table_name")

Data Security: Enforcing Column Level Encryption
pip install cryptography
from cryptography.fernet import Fernet
key = Fernet.generate_key()
f = Fernet(key)

mail_id = "abd123@gmail.com"
test = f.encrypt(mail_id)    - f.decrypt(mail_id)
print(test)

PYSPARK FUNCTION = SPLIT
pyspark.sql.functions.split
DF.withColumn('Frist_name',split(DF['Original_col_name'],'').getiteam(0)).withColumn('Lat_name',split(DF['Original_col_name'],'').getiteam(1))

Pyspark Functions| Arrays_zip
from pyspark.sql import functions as F
outputdf = inputdf.withColumn("col_name",F.arrays_zip("col1","col2"))
df = df.withColumn('Explode',F.explode(col_name))
df = df.withColumn('emp_name',df_brand_exp['Explode.employee.emp_name']).drop("explode")

Null Count of Each Column in Dataframe
from pyspark.sql.functions import col,count,when
df1 = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])

Delta Lake Audit Log Table with Operation Metrics
df.select.withColumn('col_name',lit(dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()))
         .withColumn('col_name',lit(dbutils.notebook.entry_point.getDbutils().notebook().getContect().notebookPath().get()))
         .withColumn('col_name',lit(current_timestamp))

PYSPARK PIVOT
from pyspark.sql import SparkSession
from pyspark.sql import Row

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("Pivot Example") \
    .getOrCreate()

# Sample data
data = [
    Row(date="2023-09-01", region="North", product="Product_A", sales=100),
    Row(date="2023-09-01", region="North", product="Product_B", sales=200),
    Row(date="2023-09-01", region="South", product="Product_A", sales=150),
    Row(date="2023-09-02", region="North", product="Product_A", sales=300),
    Row(date="2023-09-02", region="South", product="Product_B", sales=400),
    Row(date="2023-09-02", region="North", product="Product_B", sales=250)
]

# Create DataFrame
df = spark.createDataFrame(data)

# Show the DataFrame
df.show()
+----------+------+---------+-----+
|      date|region|  product|sales|
+----------+------+---------+-----+
|2023-09-01| North|Product_A|  100|
|2023-09-01| North|Product_B|  200|
|2023-09-01| South|Product_A|  150|
|2023-09-02| North|Product_A|  300|
|2023-09-02| South|Product_B|  400|
|2023-09-02| North|Product_B|  250|
+----------+------+---------+-----+
# Pivot the DataFrame
pivot_df = df.groupBy("date", "region").pivot("product").sum("sales")

# Show the pivoted DataFrame
pivot_df.show()
+----------+------+---------+---------+
|      date|region|Product_A|Product_B|
+----------+------+---------+---------+
|2023-09-02| North|      300|      250|
|2023-09-01| South|      150|     null|
|2023-09-01| North|      100|      200|
|2023-09-02| South|     null|      400|
+----------+------+---------+---------+










