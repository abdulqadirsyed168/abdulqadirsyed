COMMAND TO CHECK SPARK WEB URL
sc.uiWebUrl

SPARK SUBMIT COMMAND
./bin/spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  [application-arguments]
  
SPARK SUMIT COMMAND YARN
./bin/spark2-submit \
   --master yarn \
   --deploy-mode cluster \
   --driver-memory 8g \
   --executor-memory 16g \
   --executor-cores 2  \
   --class org.apache.spark.examples.SparkPi \
   /spark-home/examples/jars/spark-examples_versionxx.jar 80
   

# RUNNING SPARK APPLICATION ON STANDALONE CLUSTER
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://192.168.231.132:7077 \
  --deploy-mode cluster \
  --executor-memory 5G \
  --executor-cores 8 \
  /spark-home/examples/jars/spark-examples_versionxx.jar 80


CREATE SPARK CONTEXT
conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf)

CREATE SPARK SESSION
from pyspark.sql import SparkSession
spark = SparkSession.builder.master(local[*]).appName().config().getOrCreate()

ENABLE HIVE IN SPARK
spark = SparkSession.builder()
      .master("local[1]")
      .appName("SparkByExamples.com")
      .config("spark.sql.warehouse.dir", "<path>/spark-warehouse")
      .enableHiveSupport()
      .getOrCreate();
  
 SPARK SUBMIT AFTER CREATING ALL CODE IN .py file and we need to run in terminal CMD promt spark-submit
 spark-submit script.py &> results.txt

HOW MANY WAYS CAN WE CREATE RDD

rdd = sc.parallelize(range(10))
rrd1 = spark.sparkContext.parallelize(range(10))

HOW MANY WAYS CAN WE ABLE TO CONVERT DF TO RDD
r1 = df.rdd

HOW MANY WAYS CAN WE ABLE TO CONVERT RDD TO DF

df = rdd1.toDF(schema = [])
df = spark.createDataFrame(rdd1, schema = [])

TWO WAYS WE CAN ABLE TO CREATE DATAFRAME
df = spark.createDataFrame(var1, schema = [])
df = sqlContext.createDataFrame

TO CONVERT DATAFRAME TO SQL
df.createOrReplaceTempView('table_name')

READ DIFFERENT FILE FORMAT IN PYSPARK

Books_df = spark.read.format("xml").option("rootTag", "books").option("rowTag", "book").load("dbfs:/FileStore/Tables/XmlFolder/Books.xml")-----XML FORMAT
Filename_df = spark.read.json("/tmp/Filename.type")-------JSON SINGLE LINE FORMAT
Filename_df = spark.read.option("multiline", "true").json("/tmp/Filename.type")---------------JSON MULTI LINE FORMAT
userdata2_df = spark.read.parquet("dbfs:/FileStore/Tables/ParquetFolder/userdata2.parquet")






dbutils.fs.mkdirs("/FileStore/Tables/TextFolder") #MAKE Directory

# This two ways we can able to read a file 
Emp_df = spark.read.csv("dbfs:/FileStore/Tables/TextFolder/emp.txt", header=True, inferSchema=True)
Dep_df = spark.read.option("header",True).option("inferSchema",True).csv("dbfs:/FileStore/Tables/TextFolder/dep.txt")


from pyspark.sql.functions import col
from pyspark.sql.functions import *
from pyspark.sql.types import StructType,StructField,StructType,IntegerType,StringType
from pyspark.sql.window import Window



Dep_df.select("*").show()
Emp_df.select("EMPLOYEE_ID","FIRST_NAME").show()
Emp_df.select(Emp_df.EMPLOYEE_ID,Emp_df.FIRST_NAME).show()
Emp_df.select(Emp_df['EMPLOYEE_ID'],Emp_df['FIRST_NAME']).show()
Emp_df.select(col("employee_id"),col("first_name")).show()
Emp_df.select(col("employee_id").alias("Emp_id"),col("first_name").alias("F_name")).show()

Emp_df.select("employee_id","first_name","salary").withColumn("New_salary",col("SALARY")+1000).show()
Emp_df.withColumn("New_salary",col("salary")+1000).select("employee_id","first_name","new_salary").show()
Emp_df.withColumn("salary",col("salary") - 1000).select("employee_id","first_name","salary").show()
Emp_df.withColumnRenamed("salary","emp_salary").show()

Emp_df.drop("commission_pct").show()
Emp_df.filter(col("salary") < 5000).show()
Emp_df.filter(col("salary") > 5000).show(100)
Emp_df.filter(col("salary") < 5000).select("employee_id","first_name","last_name","salary").show(100)
Emp_df.filter(col("department_id") == 50).select("employee_id","first_name","last_name","salary").show(100)
Emp_df.filter("department_id <> 50").select("employee_id","first_name","last_name","salary","department_id").show(100)
Emp_df.filter("department_id != 50").select("employee_id","first_name","last_name","salary","department_id").show(100)
Emp_df.filter("department_id != 50").filter(col("salary") > 5000).select("employee_id","first_name","last_name","salary","department_id").show(100)
Emp_df.filter("department_id == 50 and salary > 5000").select("employee_id","first_name","last_name","salary","department_id").show(100)

Emp_df.distinct().show(100)
Emp_df.dropDuplicates().show(100)
Emp_df.dropDuplicates(["department_id", "hire_date"]).show(100)
Emp_df.dropDuplicates(["department_id", "hire_date"]).select("employee_id","department_id","hire_date").show(100)


Emp_df.count()
Emp_df.select(count("salary")).show()
Emp_df.select(count("salary").alias("Total_count")).show()
Emp_df.select(max("salary").alias("Max_salary")).show()
Emp_df.select(min("salary").alias("Min_salary")).show()
Emp_df.select(avg("salary").alias("Avg_salary")).show()
Emp_df.select(sum("salary").alias("Sum_salary")).show()

Emp_df.select("*").orderBy("salary").show()
Emp_df.select("*").orderBy(col("salary").desc()).show()
Emp_df.select("*").orderBy(col("salary").desc(),col("department_id").asc()).show()
Emp_df.select("EMPLOYEE_ID","FIRST_NAME","DEPARTMENT_ID","SALARY").orderBy(col("salary").desc(),col("department_id").asc()).show()

Emp_df.groupBy("department_id").sum("salary").show(100)
Emp_df.groupBy("department_id").max("salary").show(100)
Emp_df.groupBy("department_id").min("salary").show(100)
Emp_df.groupBy("department_id","job_id").sum("salary").show(100)
Emp_df.groupBy("department_id","job_id").sum("salary","employee_id").show(100)

Emp_df.groupBy("department_id").agg(sum("salary").alias("Sum_salary"),max("salary").alias("Max_salary"),min("salary").alias("Min_salary"),
avg("salary").alias("Avg_salary")).show(100)
Emp_df.groupBy("department_id").agg(sum("salary").alias("Sum_salary"),max("salary").alias("Max_salary"),min("salary").alias("Min_salary"),
avg("salary").alias("Avg_salary")).where(col("Max_salary") >= 10000).show() 

Emp_df.withColumn("Emp_Grade", when( col("salary") > 15000, "A").when( ( col("salary") >= 10000) & ( col("salary") < 15000), "B").otherwise("C")).show(100)


Emp_df.createOrReplaceTempView("Employee")
spark.sql("select * from Employee limit 5").show()
spark.sql("select department_id,sum(salary) as total_salary from Employee group by department_id").show()
spark.sql("select employee_id, department_id, rank() over(partition by department_id order by salary desc) as rank_salary from employee").show()

Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "inner").show()
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "inner").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "left").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "right").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
Emp_df.join(Dep_df, Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID, "fullouter").select(Emp_df.EMPLOYEE_ID,Emp_df.DEPARTMENT_ID,Dep_df.DEPARTMENT_NAME).show(100)
from pyspark.sql.functions import broadcast
Emp_df.join(broadcast((Dep_df), Emp_df.DEPARTMENT_ID == Dep_df.DEPARTMENT_ID).show()


windowSpec = Window.partitionBy("DEPARTMENT_ID").orderBy("SALARY")
Emp_df.withColumn("salary_rank", rank().over(windowSpec)).select("DEPARTMENT_ID","SALARY","salary_rank").show(100)

windowSpec = Window.partitionBy("DEPARTMENT_ID").orderBy("SALARY")
Emp_df.withColumn("sum", sum("SALARY").over(windowSpec)).select("DEPARTMENT_ID","SALARY","sum").show(100)

windowSpec = Window.partitionBy("DEPARTMENT_ID")
Emp_df.withColumn("sum", sum("SALARY").over(windowSpec)).select("DEPARTMENT_ID","SALARY","sum").show(100)


FILTER CONDITIONS

display(name_df.filter(name_df.columnname ==, >, <, >=, <=, != ))

display(name_df.filter((name_df.columnname =="string value") & (name_df.columnname == "string value")) ) #AND
display(name_df.filter((name_df.columnname =="string value") | (name_df.columnname == "string value")) ) #OR

display(name_df.filter((name_df.columnname.startswith("value"))) ) 
display(name_df.filter((name_df.columnname.endswith("value"))) )
display(name_df.filter((name_df.columnname.contains("value"))) ) 

display(name_df.filter((name_df.columnname.isNull())) )
display(name_df.filter((name_df.columnname.isNotNull())) )

display(name_df.filter((name_df.columnname.isin(1,2,3))) )
display(name_df.filter((~name_df.columnname.isin())) )

display(name_df.filter((name_df.columnname.like("%stringname%"))) )
display(name_df.filter((name_df.columnname.like("%stringname"))) )
display(name_df.filter((name_df.columnname.like("stringname%"))) )





